{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ea9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba892ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should first work out a class of MDP\n",
    "class MDP:\n",
    "    \n",
    "    def __init__(self, T, V, A, B, C, D, E, d, eta, omega, alpha, beta, num_policies, num_factors):\n",
    "        '''\n",
    "        Inputs:\n",
    "            T: number of time steps\n",
    "            V: allowable (deep) policies\n",
    "            A: state-outcome mapping\n",
    "            B: transition probabilities\n",
    "            C: preferred states\n",
    "            D: priors over initial states\n",
    "            E: prior over policies\n",
    "            eta: learning rate\n",
    "            omega: forgetting rate\n",
    "            alpha: action precision\n",
    "            beta: expected free energy precision\n",
    "            num_policies: number of policies\n",
    "            num_factors: number of state factors\n",
    "        '''\n",
    "        self.T = T\n",
    "        self.V = V\n",
    "        \n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.D = D\n",
    "        self.E = E\n",
    "        \n",
    "        self.d = d\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.omega = omega\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.num_policies = num_policies\n",
    "        self.num_factors = num_factors\n",
    "    \n",
    "    @classmethod\n",
    "    def explore_exploit_model(cls):\n",
    "        \n",
    "        # Here we specify 3 time points.\n",
    "        T = 3\n",
    "        \n",
    "        # Here we specify prior probabilities about initial states in the generative process (D).\n",
    "        # --------------------------------------------------------------------------\n",
    "        # For the 'context' state factor, we specify that the 'left better' context is the true context.\n",
    "        # For the 'behavior' state factor, we specify that the agent always starts a trial with 'start' state.\n",
    "        D = [0, 0]\n",
    "        D[0] = np.array([[1], [0]])\n",
    "        D[1] = np.array([[1], [0], [0], [0]])\n",
    "        \n",
    "        # --------------------------------------------------------------------------\n",
    "        # We can also specify prior beliefs about initial states (d)\n",
    "        d = [0,0]\n",
    "        # We assume that the agent starts out with equal beliefs in the 'context' state\n",
    "        d[0] = np.array([\n",
    "            [0.25], # left better\n",
    "            [0.25], # right better\n",
    "        ])\n",
    "        \n",
    "        # For behavior state, we always assume that the agent start out in the 'start' state.\n",
    "        d[1] = np.array([\n",
    "            [1], # start\n",
    "            [0], # hint\n",
    "            [0], # choose-left\n",
    "            [0], # choose-right\n",
    "        ])\n",
    "               \n",
    "        # Here we specify the probabilities of outcomes given each state in the generative process (A)\n",
    "        # --------------------------------------------------------------------------\n",
    "        # First we specify the mapping from states to observed hint (modality 1).\n",
    "        A = [0,0,0]\n",
    "        Ns = [0, 0]\n",
    "        Ns[0] = D[0].shape[0]\n",
    "        Ns[1] = D[1].shape[0]\n",
    "        \n",
    "        A0 = np.zeros((4, 3, 2))\n",
    "        for i in range(Ns[1]):\n",
    "            A0[i] = np.array([\n",
    "                [1, 1],  # No hint\n",
    "                [0, 0],  # Machine-left hint\n",
    "                [0, 0],  # Machine-right hint\n",
    "            ])\n",
    "            \n",
    "        # Notice that 'hint' behaviror states generates a hint that either the left or right slot machine is better.\n",
    "        # In this case, the hints are accturate with a probability pHA.\n",
    "        pHA = 1\n",
    "        A0[1] = np.array([\n",
    "            [0, 0],  # No hint\n",
    "            [pHA, 1-pHA], # Left slot machine is better\n",
    "            [1-pHA, pHA], # Right slot machine is better\n",
    "        ])\n",
    "        \n",
    "        A[0] = A0\n",
    "        \n",
    "        # We then specify the mapping between states and wins/losses.\n",
    "        # The first two behaviors states ('start' and 'hint') do not generate outcomes\n",
    "        A1 = np.zeros((4, 3, 2))\n",
    "        for i in range(2):\n",
    "            A1[i] = np.array([\n",
    "                [1, 1],  # Null\n",
    "                [0, 0],  # Loss\n",
    "                [0, 0],  # Win\n",
    "            ])\n",
    "        \n",
    "        # Choosing the left machine (behavior state 3) generates wins with probabily p_win\n",
    "        p_win = 0.8\n",
    "        A1[2] = np.array([\n",
    "            [0, 0],  # Null\n",
    "            [1-p_win, p_win], # Loss\n",
    "            [p_win, 1-p_win], # Win\n",
    "        ])\n",
    "        \n",
    "        # Choosing the right machine (behavior state 4) generates wins with probability p_win,\n",
    "        # with reverse mapping to context states from choosing the left machine\n",
    "        A1[3] = np.array([\n",
    "            [0, 0],  # Null\n",
    "            [p_win, 1-p_win], # Loss\n",
    "            [1-p_win, p_win], # Win\n",
    "        ])\n",
    "        \n",
    "        A[1] = A1\n",
    "        \n",
    "        # Finally. we specify the mapping between behavior states and observed behaviors.\n",
    "        A2 = np.zeros((4, 4, 2))\n",
    "        for i in range(Ns[1]):\n",
    "            A2[i] = np.zeros((4, 2))\n",
    "            A2[i][i] = np.array([1, 1])\n",
    "        \n",
    "        A[2] = A2\n",
    "        \n",
    "        # Here we specify the probalistic transitions between hidden states under each action (B).\n",
    "        # -------------------------------------------------------------------------\n",
    "        # Columns are states at time t, rows are states at time t+1.\n",
    "        B = [0, 0]\n",
    "        \n",
    "        # The agent cannot control the context state, so there is only 1 'action',\n",
    "        # indicating that contexts remain stable within a trial.\n",
    "        B0 = np.array([[1,0],[0,1]])\n",
    "        B[0] = B0\n",
    "        \n",
    "        # The agent can control the behavior state, we have 4 possible actions:\n",
    "        # 1. Move to the start state from any other state.\n",
    "        # 2. Move to the Hint state from any other state.\n",
    "        # 4. Move to the Choose Left state from any other state.\n",
    "        # 5. Move to the Choose Right state from any other state.\n",
    "        B1 = np.zeros((4,4,4))\n",
    "        for i in range(Ns[1]):\n",
    "            B1[i] = np.zeros((4,4))\n",
    "            B1[i][i] = np.array([1,1,1,1])\n",
    "        \n",
    "        B[1] = B1\n",
    "        \n",
    "        # We here specify the 'prior preferences' (C), encoded here as log probabilities.\n",
    "        # ---------------------------------------------------------------------------\n",
    "        # One matrix per outcome modality. Each row is an observation, each column is a time point.\n",
    "        No = [A[0].shape[1], A[1].shape[1], A[2].shape[1]] # number of outcomes in each outcome modality\n",
    "        C = [0, 0, 0]\n",
    "        \n",
    "        # We start by setting 0 preference for all outcomes\n",
    "        C[0] = np.zeros((No[0], T)) # hints\n",
    "        C[1] = np.zeros((No[1], T)) # wins/losses\n",
    "        C[2] = np.zeros((No[2], T)) # observed behaviors\n",
    "        \n",
    "        # Then we can specify a 'loss aversion' magnitude (la) at time points 2 \n",
    "        # and 3, and a 'reward seeking' (or 'risk-seeking') magnitude (rs). Here,\n",
    "        # rs is divided by 2 at the third time point to encode a smaller win ($2\n",
    "        # instead of $4) if taking the hint before choosing a slot machine.\n",
    "        la = 1\n",
    "        rs = 4\n",
    "        C[1] = np.array([\n",
    "            [0, 0, 0], # null\n",
    "            [0, -la, -la], # loss\n",
    "            [0, rs, rs/2], # win\n",
    "        ])\n",
    "\n",
    "        # Here we specify the the policies (V) .\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Here we specify the the policies (V) .\n",
    "        # Each policy is just a sequence of actions.\n",
    "        # In our case, rows correspond to time points.\n",
    "        num_policies = 5 # number of policies\n",
    "        num_factors = 2 # number of factors\n",
    "        \n",
    "        V = [0, 0]\n",
    "        \n",
    "        V[0] = np.array([\n",
    "            [1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1], \n",
    "        ]) # context state is not controllable\n",
    "        \n",
    "        V[1] = np.array([\n",
    "            [1, 2, 2, 3, 4],\n",
    "            [1, 3, 4, 1, 1],\n",
    "        ])\n",
    "        \n",
    "        # Here we specify the habits of the agent (E).\n",
    "        # -------------------------------------------------------------------------------\n",
    "        # Here we specify the habits of the agent (E).\n",
    "        # We will not equip the agent with habits with any starting habits.\n",
    "        E = [np.ones((5, 1))]\n",
    "        \n",
    "        ## Here we specify all other constants\n",
    "        ## -------------------------------------------------------------------------------\n",
    "        # Learning rate\n",
    "        eta = 1\n",
    "        \n",
    "        # Forgetting rate\n",
    "        omega = 1\n",
    "        \n",
    "        # Expected precision of expected free energy (G) over policies\n",
    "        beta = 1\n",
    "        \n",
    "        # Alpha: An 'inverse temperature' or 'action precision' parameter that\n",
    "        # controls how much randomness there is when selecting actions.\n",
    "        alpha = 32\n",
    "        \n",
    "        return cls(T, V, A, B, C, D, E, d, eta, omega, alpha, beta, num_policies, num_factors)\n",
    "    \n",
    "    def message_passing_and_policy_selection(self):\n",
    "        \n",
    "        # Store initial parameter values of generative model for free energy calculations.\n",
    "        # -------------------------------------------------------------------------------\n",
    "        # 'Complexity' of d vector concentration parameters\n",
    "        d_prior = [0 for i in range(self.num_factors)]\n",
    "        d_complexity = [0 for i in range(self.num_factors)]\n",
    "        if hasattr(self, 'd'):\n",
    "            for i in range(self.num_factors):\n",
    "                d_prior[i] = self.d[i]\n",
    "                d_complexity[i] = self._spm_wnorm(d_prior[i])\n",
    "        \n",
    "        # We here normalize the matrices, so that they can actually be treated as probabilities\n",
    "        # -------------------------------------------------------------------------------\n",
    "        # Normalize A matrix\n",
    "        self._col_norm(self.A)\n",
    "        \n",
    "        # Normalize B matrix\n",
    "        self._col_norm(self.B)\n",
    "        \n",
    "        # Normalize C matrix\n",
    "        for i in range(len(self.C)):\n",
    "            self.C[i] = self.C[i] + 1/32;\n",
    "            for t in range(self.T):\n",
    "                self.C[i][:, t] = np.log(np.exp(self.C[i][:, t])/np.sum(np.exp(self.C[i][:, t]))+np.exp(-16))\n",
    "        \n",
    "        # Normalize D matrix \n",
    "        if hasattr(self, 'd'):\n",
    "            self._col_norm(self.d)\n",
    "        else:\n",
    "            self._col_norm(self.D)\n",
    "            \n",
    "        # Normalize E vector\n",
    "        self.E = self.E/np.sum(self.E)\n",
    "        \n",
    "        # We here initialize variables.\n",
    "        # -------------------------------------------------------------------------------\n",
    "        num_modalities = len(self.A)\n",
    "        num_states = [0 for i in range(self.num_factors)] # the number of hidden states\n",
    "        num_controllable_transitions = [0 for i in range(self.num_factors)] # number of hidden controllable hidden states for each factor\n",
    "        for i in range(self.num_factors):\n",
    "            if len(self.B[i].shape) == 2:\n",
    "                num_states[i] = self.B[i].shape[0]\n",
    "                num_controllable_transitions[i] = 1\n",
    "            elif len(self.B[i].shape) > 2:\n",
    "                num_states[i] = self.B[i].shape[1]\n",
    "                num_controllable_transitions[i] = self.B[i].shape[0]\n",
    "            else:\n",
    "                print(\"The rank of matrix B is not correct\")\n",
    "        \n",
    "        # Initialize the approximate posterior over states given policies for\n",
    "        # each factor as a flat distribution over states at each time point.\n",
    "        state_posterior = []\n",
    "        for i in range(self.num_factors):\n",
    "            state_posterior.append(np.ones((self.num_policies, num_states[i], self.T))/num_states[i])\n",
    "    \n",
    "        # Initialize the approximate posterior over policies as a flat distribution over policies at each time point\n",
    "        policy_posteriors = np.ones((self.num_policies, self.T))/self.num_policies\n",
    "        \n",
    "        # Initialize posterior over actions\n",
    "        chosen_action = np.zeros((len(self.B), self.T-1))\n",
    "        \n",
    "        # if there is only one policy\n",
    "        for i in range(self.num_factors):\n",
    "            if num_controllable_transitions[i] == 1:\n",
    "                chosen_action[i,:] = np.ones((1, self.T-1))\n",
    "        setattr(self, 'chosen_action', chosen_action)\n",
    "        \n",
    "        # Intialize expected free energy precision (beta)\n",
    "        posterior_beta = 1\n",
    "        gamma = []\n",
    "        gamma.append(1/posterior_beta) # expected free energy precision\n",
    "        \n",
    "        # Messgae passing variables\n",
    "        time_const = 4 # time constant for gradient descent\n",
    "        num_iterations = 16 # number of message passing iterations\n",
    "        \n",
    "        # We here finally come to perform message passing and policy selection.\n",
    "        # -------------------------------------------------------------------------------\n",
    "        \n",
    "        # Here we first initialize all the matrices we are going to use\n",
    "        true_states = np.zeros((self.num_factors, self.T))\n",
    "        outcomes = np.zeros((num_modalities, self.T))\n",
    "        O = [[0 for i in range(num_modalities)] for j in range(self.T)]\n",
    "        normalized_firing_rates = [np.zeros((self.T, self.T, self.num_policies, num_iterations, num_states[0])),np.zeros((self.T, self.T, self.num_policies, num_iterations, num_states[1]))]\n",
    "        prediction_error = [np.zeros((self.T, self.T, self.num_policies, num_iterations, num_states[0])),np.zeros((self.T, self.T, self.num_policies, num_iterations, num_states[1]))]\n",
    "        Ft = np.zeros((self.T, self.num_factors, self.T, num_iterations)) # variational free energy at each time point\n",
    "        F = np.zeros((self.num_policies, self.T)) # varational free energy\n",
    "        G = np.zeros((self.num_policies, self.T))\n",
    "        expected_states = [0 for i in range(self.num_factors)]\n",
    "        \n",
    "        for t in range(self.T): # loop over time points\n",
    "            \n",
    "            # sample generative process\n",
    "            # -------------------------------------------------------------------------------\n",
    "            \n",
    "            # Here we sample from the prior distribution over states to obtain the state at each time point.\n",
    "            for factor in range(self.num_factors):\n",
    "                prob_state = 0\n",
    "                if t == 0:\n",
    "                    prob_state = self.D[factor]\n",
    "                elif t > 0:\n",
    "                    if factor == 0:\n",
    "                        j = int(self.chosen_action[factor, t-1])-1\n",
    "                        prob_state = np.reshape(self.B[factor][:,j], (-1, 1))\n",
    "                    elif factor > 0:\n",
    "                        i = int(self.chosen_action[factor, t-1])-1\n",
    "                        k = int(true_states[factor, t-1])-1\n",
    "                        prob_state = np.reshape(self.B[factor][i,:,k], (-1, 1))        \n",
    "                prob_state = np.reshape(np.cumsum(prob_state, axis=0), (-1))\n",
    "                index_to_use = np.where(prob_state > np.random.random())[0][0]\n",
    "                true_states[factor, t] = prob_state[index_to_use]\n",
    "            \n",
    "            # Here we sample observations\n",
    "            num_modalities = len(self.A)\n",
    "            for modality in range(num_modalities):\n",
    "                i = int(true_states[1, t]) - 1\n",
    "                k = int(true_states[0, t]) - 1\n",
    "                index_to_use = np.where(np.cumsum(self.A[modality][i, :, k].reshape((-1,1)), axis=0).reshape(-1)>np.random.random())[0][0]\n",
    "                outcomes[modality, t] = np.cumsum(self.A[modality][i, :, k].reshape((-1,1)), axis=0)[index_to_use]\n",
    "            \n",
    "            # Here we express observations as a structure containing 1 x observations vector for each\n",
    "            # modality with a 1 in the position corresponding to the observation received on the trial\n",
    "            for modality in range(num_modalities):\n",
    "                vec = np.zeros((1, self.A[modality].shape[1]))\n",
    "                index = int(outcomes[modality, t])-1\n",
    "                vec[0, index] = 1\n",
    "                O[modality][t] = vec\n",
    "            \n",
    "            # Marginal message passing (minimize F and infer posterior over states)\n",
    "            # -------------------------------------------------------------------------------\n",
    "            for policy in range(self.num_policies):\n",
    "                for Ni in range(num_iterations):\n",
    "                    for factor in range(self.num_factors):\n",
    "                        lnAo = np.zeros(state_posterior[factor].shape) # initialise matrix containing the log likelihood of observations\n",
    "                        for tau in range(self.T):\n",
    "                            v_depolarization = self._nat_log(state_posterior[factor][policy, :, tau]).reshape(-1,1) # convert approximate posteriors into depolarisation variable v\n",
    "                            if tau < t+1:\n",
    "                                for modal in range(num_modalities):\n",
    "                                    lnA = np.transpose(np.expand_dims(self._nat_log(self.A[modal][:,int(outcomes[modal,tau]-1),:]), axis=1), [1, 2, 0])\n",
    "                                    for fj in range(self.num_factors):\n",
    "                                        if fj != factor:\n",
    "                                            lnAs = self._md_dot(np.squeeze(lnA, axis=0), state_posterior[fj][0, :, tau].reshape(-1, 1), fj)\n",
    "                                            lnA = lnAs\n",
    "                                    lnAo[0, :, tau] = lnAo[0, :, tau] + lnA.reshape(-1)\n",
    "                            \n",
    "                            # 'forwards' and 'backwards' messages at each tau\n",
    "                            if tau == 0: # first tau\n",
    "                                lnD = self._nat_log(self.d[factor]).reshape(-1,1) # forwards message\n",
    "                                if factor == 0:\n",
    "                                    lnBs = self._nat_log(np.matmul(self._B_norm(self.B[factor][:,:].T),state_posterior[factor][policy,:,tau+1])).reshape(-1,1) # backwards message\n",
    "                                else:\n",
    "                                    lnBs = self._nat_log(np.matmul(self._B_norm(self.B[factor][self.V[factor][tau,policy]-1,:,:].T), state_posterior[factor][policy,:,tau+1])).reshape(-1,1)\n",
    "                            \n",
    "                            elif tau == self.T-1: # last tau\n",
    "                                if factor == 0:\n",
    "                                    lnD = self._nat_log(np.matmul(self.B[0][:,:], state_posterior[factor][policy,:,tau-1])).reshape(-1,1) # forwards message\n",
    "                                else:\n",
    "                                    lnD = self._nat_log(np.matmul(self.B[factor][self.V[factor][tau-1,policy]-1,:,:], state_posterior[factor][policy,:,tau-1])).reshape(-1,1)\n",
    "                                lnBs = np.zeros(self.d[factor].shape)\n",
    "                            \n",
    "                            else: # T-1 > tau > 0\n",
    "                                if factor == 0:\n",
    "                                    lnD = self._nat_log(np.matmul(self.B[0][:,:], state_posterior[factor][policy,:,tau-1])).reshape(-1,1) # forwards message\n",
    "                                    lnBs = self._nat_log(np.matmul(self._B_norm(self.B[factor][:,:].T),state_posterior[factor][policy,:,tau+1])).reshape(-1,1) # backwards message\n",
    "                                else:\n",
    "                                    lnD = self._nat_log(np.matmul(self.B[factor][self.V[factor][tau-1,policy]-1,:,:], state_posterior[factor][policy,:,tau-1])).reshape(-1,1)\n",
    "                                    lnBs = self._nat_log(np.matmul(self._B_norm(self.B[factor][self.V[factor][tau,policy]-1,:,:].T), state_posterior[factor][policy,:,tau+1])).reshape(-1,1)\n",
    "                                    \n",
    "                            # we then combine both the messages and do a gradient descent on the posterior\n",
    "                            v_depolarization = v_depolarization + (0.5*lnD + 0.5*lnBs + lnAo[0,:,tau].reshape(-1,1) - v_depolarization)/time_const\n",
    "                            #print(v_depolarization)\n",
    "                            # variational free energy at each time point\n",
    "                            a = state_posterior[factor][policy,:,tau]\n",
    "                            b = (0.5*lnD+0.5*lnBs-lnAo[0,:,tau].reshape(-1,1)).reshape(-1)-self._nat_log(state_posterior[factor][policy,:,tau])\n",
    "                            Ft[t, factor, tau, Ni] = np.dot(a,b)\n",
    "                            # update posterior by running v through a softmax\n",
    "                            state_posterior[factor][policy,:,tau] = ((np.exp(v_depolarization))/np.sum(np.exp(v_depolarization), axis=0)[0]).reshape(-1)\n",
    "                            # store state_positerior from each epoch of gradient descent for each tau\n",
    "                            normalized_firing_rates[factor][tau,t,policy,Ni,:] = state_posterior[factor][policy,:,tau]\n",
    "                            # store v from each epoch of gradient descent for each tau\n",
    "                            prediction_error[factor][tau,t,policy,Ni,:] = v_depolarization.reshape(-1)\n",
    "                \n",
    "                # variational free energy for each policy\n",
    "                F_intermidiate = np.sum(Ft, axis=1) # sum over state factors\n",
    "                F_intermidiate = np.squeeze(np.sum(F_intermidiate, 1)).T # sum over tau then squeeze into 16x3 matrix\n",
    "                # store the value of the message pass at the last iteration into the variational free energy\n",
    "                F_intermidiate_flatten = F_intermidiate.flatten()\n",
    "                F[policy, t] = F_intermidiate_flatten[np.flatnonzero(F_intermidiate)[-1]]\n",
    "            \n",
    "            # Expected free energy (G) under each policy\n",
    "            # -------------------------------------------------------------------------------\n",
    "            \n",
    "            # Initialize intermediate expected free energy variable for each policy\n",
    "            G_intermediate = np.zeros((self.num_policies,1))\n",
    "            # Policy horizon for 'counterfactual rollout' for deep policies\n",
    "            horizon = self.T\n",
    "            \n",
    "            # Do the loop through policies \n",
    "            for policy in range(self.num_policies):\n",
    "                \n",
    "                # Bayesian superise about 'd'\n",
    "                if hasattr(self, 'd'):\n",
    "                    for factor in range(self.num_factors):\n",
    "                        G_intermediate[policy] = G_intermediate[policy] - np.dot(d_complexity[factor].reshape(-1), state_posterior[factor][policy,:,0].reshape(-1))\n",
    "                \n",
    "                # We then come to calculate the expected free energy from time t to the policy horizon.\n",
    "                for timestep in range(t, horizon):  \n",
    "                    # store the expected states from each policy and time\n",
    "                    for factor in range(self.num_factors):\n",
    "                        expected_states[factor] = state_posterior[factor][policy,:,timestep].reshape(-1,1)\n",
    "                        \n",
    "                    # calculate Bayesian surprise then add it the expected free energy\n",
    "                    G_intermediate[policy] = G_intermediate[policy] + self._G_epistemic_value(self.A[:], expected_states[:])\n",
    "                    \n",
    "                    for modality in range(num_modalities):\n",
    "                        predictive_observations_posterior = self._cell_md_dot(self.A[modality], expected_states)\n",
    "                        G_intermediate[policy] = G_intermediate[policy] + (predictive_observations_posterior.T).dot(self.C[modality][:,t])\n",
    "            \n",
    "            G[:,t] = G_intermediate.squeeze()\n",
    "            print(G)\n",
    "                            \n",
    "                            \n",
    "    # Normalize vector columns\n",
    "    def _col_norm(self, input_matrices):\n",
    "        num_factors = len(input_matrices)\n",
    "        # output_matrices = [0 for i in range(num_factors)]\n",
    "        for i in range(num_factors):\n",
    "            num = input_matrices[i].shape\n",
    "            if num[1] == 1:\n",
    "                z = np.sum(input_matrices[i], axis=0)\n",
    "                input_matrices[i] = input_matrices[i]/z\n",
    "            else:\n",
    "                for j in range(num[0]):\n",
    "                    z = np.sum(input_matrices[i][j], axis=0)\n",
    "                    input_shape = input_matrices[i][j].shape\n",
    "                    if (input_matrices[i][j] - np.zeros(input_shape) == np.zeros(input_shape)).all():\n",
    "                        continue\n",
    "                    input_matrices[i][j] = input_matrices[i][j]/z\n",
    "            # output_matrices[i] = input_matrices[i]\n",
    "        return\n",
    "    \n",
    "    # This function substracts the inverse of each column entry\n",
    "    # from the inverse of the sum of the columns and then divide by 2.\n",
    "    def _spm_wnorm(self, input_matrix):\n",
    "        input_matrix = input_matrix + np.exp(-16)\n",
    "        input_matrix = (1/np.sum(input_matrix, axis=0) - 1/input_matrix)/2\n",
    "        return input_matrix\n",
    "    \n",
    "    # Natural log that replaces zero values with very small values for numerical reasons.\n",
    "    def _nat_log(self, x):\n",
    "        return np.log(x+np.exp(-16))\n",
    "    \n",
    "    # Dot product along dimension f\n",
    "    def _md_dot(self, A, s, f):\n",
    "        if f == 0:\n",
    "            matrix_to_return = np.matmul(A.T, s)\n",
    "        elif f == 1:\n",
    "            matrix_to_return = np.matmul(A, s)\n",
    "        return matrix_to_return\n",
    "    \n",
    "    def _cell_md_dot(self, X, x):\n",
    "        DIM = np.array([i for i in range(len(x))]).reshape(1,len(x)) + np.ndim(X) - len(x)\n",
    "        for d in range(len(x)):\n",
    "            s = [1 for i in range(np.ndim(X))]\n",
    "            s[DIM[0][d]] = np.prod(list(x[d].shape))\n",
    "            s = s[2:] + s[0:2] # because numpy and matlab has different arrangement for the shape of matrices,we have consider this fact when reshaping numpy arrays.\n",
    "            x[d] = x[d].reshape(*s)\n",
    "            X = X*(x[d].reshape(*s))\n",
    "            if DIM[0][d]%2 != 0:\n",
    "                X = np.expand_dims(np.sum(X, axis=DIM[0][d]+1), axis=DIM[0][d]+1)\n",
    "            else:\n",
    "                X = np.expand_dims(np.sum(X, axis=0), axis=0)\n",
    "            #X = np.expand_dims(np.sum(X, axis=DIM[0][d]+1), axis=DIM[0][d]+1)\n",
    "        X = X.squeeze()\n",
    "        return X\n",
    "        \n",
    "    \n",
    "    # Normalize the elements of B transpose as required by MMP\n",
    "    def _B_norm(self, B):\n",
    "        bb = B\n",
    "        z = np.sum(bb, axis=0)\n",
    "        bb = bb/z\n",
    "        np.nan_to_num(bb, copy=False, nan=0)\n",
    "        b = bb\n",
    "        return b\n",
    "    \n",
    "    # Calculate the Bayesian surprise in expected free energy\n",
    "    def _G_epistemic_value(self, A, s):\n",
    "        qx = self._spm_cross(s)\n",
    "        qx_flatten = qx.flatten()\n",
    "        G = 0\n",
    "        qo = 0\n",
    "        for i in np.where(qx_flatten>np.exp(-16))[0]:\n",
    "            po = 1\n",
    "            for g in range(len(A)):\n",
    "                A_list = [A[g][i][:,j] for i in range(A[g].shape[0]) for j in range(A[g].shape[-1])]\n",
    "                po = np.kron(po,A_list[i])\n",
    "            po = po[:]\n",
    "            qo = qo + qx_flatten[i]*po\n",
    "            G = G + qx_flatten[i]*po.T.dot(self._nat_log(po))\n",
    "        G = G - qo.T.dot(self._nat_log(qo))\n",
    "        return G\n",
    "        \n",
    "    # The outer product of matrices.\n",
    "    # This method is buggy when dealing with 1-d arrays.\n",
    "    def _spm_cross(self, *args):\n",
    "        \n",
    "        # dealing with the single case\n",
    "        def helper_single(*args):\n",
    "            if len(args) == 1:\n",
    "                matrices = args[0]\n",
    "                extension_dims = [1 for i in range(len(args[0]))]\n",
    "                if isinstance(matrices, np.ndarray):\n",
    "                    Y = matrices\n",
    "                elif len(matrices) <= 1:\n",
    "                    Y = matrices[0]\n",
    "                else:\n",
    "                    Z = matrices.pop(0)\n",
    "                    Z_shape = list(Z.shape)\n",
    "                    if sum(extension_dims) == fixed_length:\n",
    "                        matrices = matrices[::-1]\n",
    "                    W = helper_single(*[matrices])\n",
    "                    W_shape = list(W.shape)\n",
    "                    W = W.reshape(*W_shape,*list(np.ones(np.ndim(Z),dtype=int)))\n",
    "                    Y = np.kron(W,Z)\n",
    "                return Y.squeeze()\n",
    "        \n",
    "        # dealing with multiple inputs\n",
    "        def helper_multiple(*args):\n",
    "            Y_list = []\n",
    "            for i in range(len(args)):\n",
    "                matrices = args[i]\n",
    "                extension_dims = [1 for i in range(len(matrices))]\n",
    "                if isinstance(matrices, np.ndarray):\n",
    "                    Y = matrices\n",
    "                elif len(matrices) <= 1:\n",
    "                    Y = matrices[0]\n",
    "                else:\n",
    "                    Z = matrices.pop(len(matrices)-1)\n",
    "                    Z_shape = list(Z.shape)\n",
    "                    if sum(extension_dims) == fixed_length_list[i]:\n",
    "                        matrices = matrices[::-1]\n",
    "                    W = helper_single(*[matrices])\n",
    "                    Z = Z.reshape(*Z.shape,*list(np.ones(np.ndim(W),dtype=int)))\n",
    "                    W_shape = list(W.shape)\n",
    "                    Y = np.kron(W,Z)\n",
    "                Y_list.append(Y)\n",
    "            V = self._spm_cross(Y_list)\n",
    "            return V\n",
    "\n",
    "        if len(args) == 1:\n",
    "            fixed_length = len(args[0])\n",
    "            return helper_single(*args)\n",
    "        else:\n",
    "            fixed_length_list = [len(args[i]) for i in range(len(args))]\n",
    "            return helper_multiple(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010e60d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDP = MDP.explore_exploit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db203603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.72660102  0.          0.        ]\n",
      " [-8.84199649  0.          0.        ]\n",
      " [-8.84199649  0.          0.        ]\n",
      " [-9.53413628  0.          0.        ]\n",
      " [-9.53413628  0.          0.        ]]\n",
      "[[ -9.72660102 -12.00513211   0.        ]\n",
      " [ -8.84199649 -10.31465811   0.        ]\n",
      " [ -8.84199649 -10.31465811   0.        ]\n",
      " [ -9.53413628 -12.00513206   0.        ]\n",
      " [ -9.53413628 -12.00513206   0.        ]]\n",
      "[[ -9.72660102 -12.00513211  -3.65102613]\n",
      " [ -8.84199649 -10.31465811  -3.6510261 ]\n",
      " [ -8.84199649 -10.31465811  -3.6510261 ]\n",
      " [ -9.53413628 -12.00513206  -3.65102613]\n",
      " [ -9.53413628 -12.00513206  -3.65102613]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyiyan/opt/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:504: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "MDP.message_passing_and_policy_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d8b6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627801c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35bdc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddc56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99495d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0876ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def spm_cross(*args):\n",
    "    \n",
    "#     def helper_single(*args):\n",
    "#         if len(args) == 1:\n",
    "#             matrices = args[0]\n",
    "#             extension_dims = [1 for i in range(len(args[0]))]\n",
    "#             if isinstance(matrices, np.ndarray):\n",
    "#                 Y = matrices\n",
    "#             elif len(matrices) <= 1:\n",
    "#                 Y = matrices[0]\n",
    "#             else:\n",
    "#                 Z = matrices.pop(0)\n",
    "#                 Z_shape = list(Z.shape)\n",
    "#                 if sum(extension_dims) == fixed_length:\n",
    "#                     matrices = matrices[::-1]\n",
    "#                 W = helper_single(*[matrices])\n",
    "#                 W_shape = list(W.shape)\n",
    "#                 W = W.reshape(*W_shape,*list(np.ones(np.ndim(Z),dtype=int)))\n",
    "#                 Y = np.kron(W,Z)\n",
    "                \n",
    "#             return Y.squeeze()\n",
    "\n",
    "#     def helper_multiple(*args):\n",
    "#         Y_list = []\n",
    "#         for i in range(len(args)):\n",
    "#             matrices = args[i]\n",
    "#             extension_dims = [1 for i in range(len(matrices))]\n",
    "#             if isinstance(matrices, np.ndarray):\n",
    "#                 Y = matrices\n",
    "#             elif len(matrices) <= 1:\n",
    "#                 Y = matrices[0]\n",
    "#             else:\n",
    "#                 Z = matrices.pop(len(matrices)-1)\n",
    "#                 Z_shape = list(Z.shape)\n",
    "#                 if sum(extension_dims) == fixed_length_list[i]:\n",
    "#                     matrices = matrices[::-1]\n",
    "#                 W = helper_single(*[matrices])\n",
    "#                 Z = Z.reshape(*Z.shape,*list(np.ones(np.ndim(W),dtype=int)))\n",
    "#                 W_shape = list(W.shape)\n",
    "#                 Y = np.kron(W,Z)\n",
    "            \n",
    "#             Y_list.append(Y)\n",
    " \n",
    "#         V = spm_cross(Y_list)\n",
    "#         return V\n",
    "              \n",
    "#     if len(args) == 1:\n",
    "#         fixed_length = len(args[0])\n",
    "#         return helper_single(*args)\n",
    "#     else:\n",
    "#         fixed_length_list = [len(args[i]) for i in range(len(args))]\n",
    "#         return helper_multiple(*args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
