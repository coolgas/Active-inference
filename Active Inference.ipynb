{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import scipy.special as sc\n",
    "import copy\n",
    "\n",
    "from scipy.special import psi"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "# Should first work out a class of MDP\n",
    "class MDP:\n",
    "    \n",
    "    def __init__(self, T, V, A, B, C, D, E, d, eta, omega, alpha, beta, num_policies, num_factors):\n",
    "        '''\n",
    "        Inputs:\n",
    "            T: number of time steps\n",
    "            V: allowable (deep) policies\n",
    "            A: state-outcome mapping\n",
    "            B: transition probabilities\n",
    "            C: preferred states\n",
    "            D: priors over initial states\n",
    "            E: prior over policies\n",
    "            eta: learning rate\n",
    "            omega: forgetting rate\n",
    "            alpha: action precision\n",
    "            beta: expected free energy precision\n",
    "            num_policies: number of policies\n",
    "            num_factors: number of state factors\n",
    "        '''\n",
    "        self.T = T\n",
    "        self.V = V\n",
    "        \n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.D = D\n",
    "        self.E = E\n",
    "        \n",
    "        self.d = d\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.omega = omega\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.num_policies = num_policies\n",
    "        self.num_factors = num_factors\n",
    "    \n",
    "    @classmethod\n",
    "    def explore_exploit_model(cls):\n",
    "        \n",
    "        # Here we specify 3 time points.\n",
    "        T = 3\n",
    "        \n",
    "        # Here we specify prior probabilities about initial states in the generative process (D).\n",
    "        # --------------------------------------------------------------------------\n",
    "        # For the 'context' state factor, we specify that the 'left better' context is the true context.\n",
    "        # For the 'behavior' state factor, we specify that the agent always starts a trial with 'start' state.\n",
    "        D = [0, 0]\n",
    "        D[0] = np.array([[1], [0]])\n",
    "        D[1] = np.array([[1], [0], [0], [0]])\n",
    "        \n",
    "        # --------------------------------------------------------------------------\n",
    "        # We can also specify prior beliefs about initial states (d)\n",
    "        d = [0,0]\n",
    "        # We assume that the agent starts out with equal beliefs in the 'context' state\n",
    "        d[0] = np.array([\n",
    "            [0.25], # left better\n",
    "            [0.25], # right better\n",
    "        ])\n",
    "        \n",
    "        # For behavior state, we always assume that the agent start out in the 'start' state.\n",
    "        d[1] = np.array([\n",
    "            [1], # start\n",
    "            [0], # hint\n",
    "            [0], # choose-left\n",
    "            [0], # choose-right\n",
    "        ], dtype=float)\n",
    "\n",
    "        # Here we specify the probabilities of outcomes given each state in the generative process (A)\n",
    "        # --------------------------------------------------------------------------\n",
    "        # First we specify the mapping from states to observed hint (modality 1).\n",
    "        A = [0,0,0]\n",
    "        Ns = [0, 0]\n",
    "        Ns[0] = D[0].shape[0]\n",
    "        Ns[1] = D[1].shape[0]\n",
    "        \n",
    "        A0 = np.zeros((4, 3, 2))\n",
    "        for i in range(Ns[1]):\n",
    "            A0[i] = np.array([\n",
    "                [1, 1],  # No hint\n",
    "                [0, 0],  # Machine-left hint\n",
    "                [0, 0],  # Machine-right hint\n",
    "            ])\n",
    "            \n",
    "        # Notice that 'hint' behaviror states generates a hint that either the left or right slot machine is better.\n",
    "        # In this case, the hints are accturate with a probability pHA.\n",
    "        pHA = 1\n",
    "        A0[1] = np.array([\n",
    "            [0, 0],  # No hint\n",
    "            [pHA, 1-pHA], # Left slot machine is better\n",
    "            [1-pHA, pHA], # Right slot machine is better\n",
    "        ])\n",
    "        \n",
    "        A[0] = A0\n",
    "        \n",
    "        # We then specify the mapping between states and wins/losses.\n",
    "        # The first two behaviors states ('start' and 'hint') do not generate outcomes\n",
    "        A1 = np.zeros((4, 3, 2))\n",
    "        for i in range(2):\n",
    "            A1[i] = np.array([\n",
    "                [1, 1],  # Null\n",
    "                [0, 0],  # Loss\n",
    "                [0, 0],  # Win\n",
    "            ])\n",
    "        \n",
    "        # Choosing the left machine (behavior state 3) generates wins with probabily p_win\n",
    "        p_win = 0.8\n",
    "        A1[2] = np.array([\n",
    "            [0, 0],  # Null\n",
    "            [1-p_win, p_win], # Loss\n",
    "            [p_win, 1-p_win], # Win\n",
    "        ])\n",
    "        \n",
    "        # Choosing the right machine (behavior state 4) generates wins with probability p_win,\n",
    "        # with reverse mapping to context states from choosing the left machine\n",
    "        A1[3] = np.array([\n",
    "            [0, 0],  # Null\n",
    "            [p_win, 1-p_win], # Loss\n",
    "            [1-p_win, p_win], # Win\n",
    "        ])\n",
    "        \n",
    "        A[1] = A1\n",
    "        \n",
    "        # Finally. we specify the mapping between behavior states and observed behaviors.\n",
    "        A2 = np.zeros((4, 4, 2))\n",
    "        for i in range(Ns[1]):\n",
    "            A2[i] = np.zeros((4, 2))\n",
    "            A2[i][i] = np.array([1, 1])\n",
    "        \n",
    "        A[2] = A2\n",
    "        \n",
    "        # Here we specify the probalistic transitions between hidden states under each action (B).\n",
    "        # -------------------------------------------------------------------------\n",
    "        # Columns are states at time t, rows are states at time t+1.\n",
    "        B = [0, 0]\n",
    "        \n",
    "        # The agent cannot control the context state, so there is only 1 'action',\n",
    "        # indicating that contexts remain stable within a trial.\n",
    "        B0 = np.array([[1,0],[0,1]])\n",
    "        B[0] = B0\n",
    "        \n",
    "        # The agent can control the behavior state, we have 4 possible actions:\n",
    "        # 1. Move to the start state from any other state.\n",
    "        # 2. Move to the Hint state from any other state.\n",
    "        # 4. Move to the Choose Left state from any other state.\n",
    "        # 5. Move to the Choose Right state from any other state.\n",
    "        B1 = np.zeros((4,4,4))\n",
    "        for i in range(Ns[1]):\n",
    "            B1[i] = np.zeros((4,4))\n",
    "            B1[i][i] = np.array([1,1,1,1])\n",
    "        \n",
    "        B[1] = B1\n",
    "        \n",
    "        # We here specify the 'prior preferences' (C), encoded here as log probabilities.\n",
    "        # ---------------------------------------------------------------------------\n",
    "        # One matrix per outcome modality. Each row is an observation, each column is a time point.\n",
    "        No = [A[0].shape[1], A[1].shape[1], A[2].shape[1]] # number of outcomes in each outcome modality\n",
    "        C = [0, 0, 0]\n",
    "        \n",
    "        # We start by setting 0 preference for all outcomes\n",
    "        C[0] = np.zeros((No[0], T)) # hints\n",
    "        C[1] = np.zeros((No[1], T)) # wins/losses\n",
    "        C[2] = np.zeros((No[2], T)) # observed behaviors\n",
    "        \n",
    "        # Then we can specify a 'loss aversion' magnitude (la) at time points 2 \n",
    "        # and 3, and a 'reward seeking' (or 'risk-seeking') magnitude (rs). Here,\n",
    "        # rs is divided by 2 at the third time point to encode a smaller win ($2\n",
    "        # instead of $4) if taking the hint before choosing a slot machine.\n",
    "        la = 1\n",
    "        rs = 4\n",
    "        C[1] = np.array([\n",
    "            [0, 0, 0], # null\n",
    "            [0, -la, -la], # loss\n",
    "            [0, rs, rs/2], # win\n",
    "        ])\n",
    "\n",
    "        # Here we specify the the policies (V) .\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Here we specify the the policies (V) .\n",
    "        # Each policy is just a sequence of actions.\n",
    "        # In our case, rows correspond to time points.\n",
    "        num_policies = 5 # number of policies\n",
    "        num_factors = 2 # number of factors\n",
    "        \n",
    "        V = [0, 0]\n",
    "        \n",
    "        V[0] = np.array([\n",
    "            [1, 1, 1, 1, 1],\n",
    "            [1, 1, 1, 1, 1], \n",
    "        ]) # context state is not controllable\n",
    "        \n",
    "        V[1] = np.array([\n",
    "            [1, 2, 2, 3, 4],\n",
    "            [1, 3, 4, 1, 1],\n",
    "        ])\n",
    "        \n",
    "        # Here we specify the habits of the agent (E).\n",
    "        # -------------------------------------------------------------------------------\n",
    "        # Here we specify the habits of the agent (E).\n",
    "        # We will not equip the agent with habits with any starting habits.\n",
    "        E = np.ones((5, 1))\n",
    "        \n",
    "        ## Here we specify all other constants\n",
    "        ## -------------------------------------------------------------------------------\n",
    "        # Learning rate\n",
    "        eta = 1\n",
    "        \n",
    "        # Forgetting rate\n",
    "        omega = 1\n",
    "        \n",
    "        # Expected precision of expected free energy (G) over policies\n",
    "        beta = 1\n",
    "        \n",
    "        # Alpha: An 'inverse temperature' or 'action precision' parameter that\n",
    "        # controls how much randomness there is when selecting actions.\n",
    "        alpha = 32\n",
    "        \n",
    "        return cls(T, V, A, B, C, D, E, d, eta, omega, alpha, beta, num_policies, num_factors)\n",
    "    \n",
    "    def message_passing_and_policy_selection(self):\n",
    "        \n",
    "        # Store values of MDP class locally\n",
    "        A = copy.deepcopy(self.A)\n",
    "        B = copy.deepcopy(self.B)\n",
    "        C = copy.deepcopy(self.C)\n",
    "        d = copy.deepcopy(self.d)\n",
    "        E = copy.deepcopy(self.E)\n",
    "\n",
    "        # Store initial parameter values of generative model for free energy calculations.\n",
    "        # -------------------------------------------------------------------------------\n",
    "        # 'Complexity' of d vector concentration parameters\n",
    "        d_complexity = [0 for i in range(self.num_factors)]\n",
    "        d_prior = copy.deepcopy(d)\n",
    "        if hasattr(self, 'd'):\n",
    "            for i in range(self.num_factors):\n",
    "                d_complexity[i] = self._spm_wnorm(d_prior[i])\n",
    "        \n",
    "        # We here normalize the matrices, so that they can actually be treated as probabilities\n",
    "        # -------------------------------------------------------------------------------\n",
    "        # Normalize A matrix\n",
    "        A = self._col_norm(self.A)\n",
    "        \n",
    "        # Normalize B matrix\n",
    "        B = self._col_norm(self.B)\n",
    "        \n",
    "        # Normalize C matrix\n",
    "        for i in range(len(self.C)):\n",
    "            C[i] = self.C[i] + 1/32;\n",
    "            for t in range(self.T):\n",
    "                C[i][:, t] = np.log(np.exp(self.C[i][:, t])/np.sum(np.exp(self.C[i][:, t]))+np.exp(-16))\n",
    "        \n",
    "        # Normalize D matrix \n",
    "        if hasattr(self, 'd'):\n",
    "            d = self._col_norm(d)\n",
    "        else:\n",
    "            d = self._col_norm(self.D)\n",
    "            \n",
    "        # Normalize E vector\n",
    "        E = self.E/np.sum(self.E)\n",
    "        \n",
    "        # We here initialize variables.\n",
    "        # -------------------------------------------------------------------------------\n",
    "        num_modalities = len(A)\n",
    "        num_states = [0 for i in range(self.num_factors)] # the number of hidden states\n",
    "        num_controllable_transitions = [0 for i in range(self.num_factors)] # number of hidden controllable hidden states for each factor\n",
    "        for i in range(self.num_factors):\n",
    "            if len(B[i].shape) == 2:\n",
    "                num_states[i] = B[i].shape[0]\n",
    "                num_controllable_transitions[i] = 1\n",
    "            elif len(B[i].shape) > 2:\n",
    "                num_states[i] = B[i].shape[1]\n",
    "                num_controllable_transitions[i] = B[i].shape[0]\n",
    "            else:\n",
    "                print(\"The rank of matrix B is not correct\")\n",
    "        \n",
    "        # Initialize the approximate posterior over states given policies for\n",
    "        # each factor as a flat distribution over states at each time point.\n",
    "        state_posterior = []\n",
    "        for i in range(self.num_factors):\n",
    "            state_posterior.append(np.ones((self.num_policies, num_states[i], self.T))/num_states[i])\n",
    "    \n",
    "        # Initialize the approximate posterior over policies as a flat distribution over policies at each time point\n",
    "        policy_posteriors = np.ones((self.num_policies, self.T))/self.num_policies\n",
    "        \n",
    "        # Initialize posterior over actions\n",
    "        chosen_action = np.zeros((len(B), self.T-1))\n",
    "        \n",
    "        # if there is only one policy\n",
    "        for i in range(self.num_factors):\n",
    "            if num_controllable_transitions[i] == 1:\n",
    "                chosen_action[i,:] = np.ones((1, self.T-1))\n",
    "        setattr(self, 'chosen_action', chosen_action)\n",
    "        \n",
    "        # Intialize expected free energy precision (beta)\n",
    "        posterior_beta = 1\n",
    "        gamma = [0. for i in range(self.T)]\n",
    "        gamma[0] = 1/posterior_beta # expected free energy precision\n",
    "        \n",
    "        # Messgae passing variables\n",
    "        time_const = 4 # time constant for gradient descent\n",
    "        num_iterations = 16 # number of message passing iterations\n",
    "        \n",
    "        # We here finally come to perform message passing and policy selection.\n",
    "        # -------------------------------------------------------------------------------\n",
    "        \n",
    "        # Here we first initialize all the matrices we are going to use\n",
    "        true_states = np.zeros((self.num_factors, self.T))\n",
    "        outcomes = np.zeros((num_modalities, self.T))\n",
    "        O = [[0 for i in range(num_modalities)] for j in range(self.T)]\n",
    "        normalized_firing_rates = [np.zeros((self.T, self.T, self.num_policies, num_iterations, num_states[0])),np.zeros((self.T, self.T, self.num_policies, num_iterations, num_states[1]))]\n",
    "        prediction_error = [np.zeros((self.T, self.T, self.num_policies, num_iterations, num_states[0])),np.zeros((self.T, self.T, self.num_policies, num_iterations, num_states[1]))]\n",
    "        Ft = np.zeros((self.T, self.num_factors, self.T, num_iterations)) # variational free energy at each time point\n",
    "        F = np.zeros((self.num_policies, self.T)) # varational free energy\n",
    "        G = np.zeros((self.num_policies, self.T))\n",
    "        expected_states = [0 for i in range(self.num_factors)]\n",
    "        policy_priors = np.zeros((self.num_policies,self.T))\n",
    "        policy_posterior = np.zeros((self.num_policies,self.T))\n",
    "        BMA_state = [0 for i in range(self.num_factors)] # Baysian model average of hidden states\n",
    "        action_posterior = np.zeros((self.T-1, 1, num_controllable_transitions[-1]))\n",
    "        gamma_update = np.zeros(self.T*num_iterations, dtype=float)\n",
    "        policy_posterior_updates = np.zeros((self.num_policies, self.T*num_iterations))\n",
    "        \n",
    "        for t in range(self.T): # loop over time points\n",
    "            \n",
    "            # sample generative process\n",
    "            # -------------------------------------------------------------------------------\n",
    "            \n",
    "            # Here we sample from the prior distribution over states to obtain the state at each time point.\n",
    "            for factor in range(self.num_factors):\n",
    "                prob_state = 0\n",
    "                if t == 0:\n",
    "                    prob_state = self.D[factor]\n",
    "                elif t > 0:\n",
    "                    if factor == 0:\n",
    "                        j = int(self.chosen_action[factor, t-1])-1\n",
    "                        prob_state = np.reshape(B[factor][:,j], (-1, 1))\n",
    "                    elif factor > 0:\n",
    "                        i = int(self.chosen_action[factor, t-1])-1\n",
    "                        k = int(true_states[factor, t-1])-1\n",
    "                        prob_state = np.reshape(B[factor][i,:,k], (-1, 1))        \n",
    "                true_states[factor,t] = np.where(np.cumsum(prob_state).reshape(-1)>=np.random.rand())[0][0]+1\n",
    "            \n",
    "            # Here we sample observations\n",
    "            num_modalities = len(A)\n",
    "            for modality in range(num_modalities):\n",
    "                i = int(true_states[1, t]) - 1\n",
    "                k = int(true_states[0, t]) - 1\n",
    "                outcomes[modality, t] = np.where(np.cumsum(A[modality][i, :, k].reshape((-1,1)), axis=0).reshape(-1)>np.random.rand())[0][0]+1\n",
    "\n",
    "            # Here we express observations as a structure containing 1 x observations vector for each\n",
    "            # modality with a 1 in the position corresponding to the observation received on the trial\n",
    "            for modality in range(num_modalities):\n",
    "                vec = np.zeros((1, A[modality].shape[1]))\n",
    "                index = int(outcomes[modality, t])-1\n",
    "                vec[0, index] = 1\n",
    "                O[modality][t] = vec\n",
    "            \n",
    "            # Marginal message passing (minimize F and infer posterior over states)\n",
    "            # -------------------------------------------------------------------------------\n",
    "            for policy in range(self.num_policies):\n",
    "                for Ni in range(num_iterations):\n",
    "                    for factor in range(self.num_factors):\n",
    "                        lnAo = np.zeros(state_posterior[factor].shape) # initialise matrix containing the log likelihood of observations\n",
    "                        for tau in range(self.T):\n",
    "                            v_depolarization = self._nat_log(state_posterior[factor][policy, :, tau]).reshape(-1,1) # convert approximate posteriors into depolarisation variable v\n",
    "                            if tau < t+1:\n",
    "                                for modal in range(num_modalities):\n",
    "                                    lnA = np.transpose(np.expand_dims(self._nat_log(A[modal][:,int(outcomes[modal,tau]-1),:]), axis=1), [1, 2, 0])\n",
    "                                    for fj in range(self.num_factors):\n",
    "                                        if fj != factor:\n",
    "                                            lnAs = self._md_dot(np.squeeze(lnA, axis=0), state_posterior[fj][0, :, tau].reshape(-1, 1), fj)\n",
    "                                            lnA = lnAs\n",
    "                                    lnAo[0, :, tau] = lnAo[0, :, tau] + lnA.reshape(-1)\n",
    "                            \n",
    "                            # 'forwards' and 'backwards' messages at each tau\n",
    "                            if tau == 0: # first tau\n",
    "                                lnD = self._nat_log(d[factor]).reshape(-1,1) # forwards message\n",
    "                                if factor == 0:\n",
    "                                    lnBs = self._nat_log(np.matmul(self._B_norm(B[factor][:,:].T),state_posterior[factor][policy,:,tau+1])).reshape(-1,1) # backwards message\n",
    "                                else:\n",
    "                                    lnBs = self._nat_log(np.matmul(self._B_norm(B[factor][self.V[factor][tau,policy]-1,:,:].T), state_posterior[factor][policy,:,tau+1])).reshape(-1,1)\n",
    "                            \n",
    "                            elif tau == self.T-1: # last tau\n",
    "                                if factor == 0:\n",
    "                                    lnD = self._nat_log(np.matmul(B[0][:,:], state_posterior[factor][policy,:,tau-1])).reshape(-1,1) # forwards message\n",
    "                                else:\n",
    "                                    lnD = self._nat_log(np.matmul(B[factor][self.V[factor][tau-1,policy]-1,:,:], state_posterior[factor][policy,:,tau-1])).reshape(-1,1)\n",
    "                                lnBs = np.zeros(d[factor].shape)\n",
    "                            \n",
    "                            else: # T-1 > tau > 0\n",
    "                                if factor == 0:\n",
    "                                    lnD = self._nat_log(np.matmul(B[0][:,:], state_posterior[factor][policy,:,tau-1])).reshape(-1,1) # forwards message\n",
    "                                    lnBs = self._nat_log(np.matmul(self._B_norm(B[factor][:,:].T),state_posterior[factor][policy,:,tau+1])).reshape(-1,1) # backwards message\n",
    "                                else:\n",
    "                                    lnD = self._nat_log(np.matmul(B[factor][self.V[factor][tau-1,policy]-1,:,:], state_posterior[factor][policy,:,tau-1])).reshape(-1,1)\n",
    "                                    lnBs = self._nat_log(np.matmul(self._B_norm(B[factor][self.V[factor][tau,policy]-1,:,:].T), state_posterior[factor][policy,:,tau+1])).reshape(-1,1)\n",
    "                                    \n",
    "                            # we then combine both the messages and do a gradient descent on the posterior\n",
    "                            v_depolarization = v_depolarization + (0.5*lnD + 0.5*lnBs + lnAo[0,:,tau].reshape(-1,1) - v_depolarization)/time_const\n",
    "                            #print(v_depolarization)\n",
    "                            # variational free energy at each time point\n",
    "                            a = state_posterior[factor][policy,:,tau]\n",
    "                            b = (0.5*lnD+0.5*lnBs-lnAo[0,:,tau].reshape(-1,1)).reshape(-1)-self._nat_log(state_posterior[factor][policy,:,tau])\n",
    "                            Ft[t, factor, tau, Ni] = np.dot(a,b)\n",
    "                            # update posterior by running v through a softmax\n",
    "                            state_posterior[factor][policy,:,tau] = ((np.exp(v_depolarization))/np.sum(np.exp(v_depolarization), axis=0)[0]).reshape(-1)\n",
    "                            # store state_positerior from each epoch of gradient descent for each tau\n",
    "                            normalized_firing_rates[factor][tau,t,policy,Ni,:] = state_posterior[factor][policy,:,tau]\n",
    "                            # store v from each epoch of gradient descent for each tau\n",
    "                            prediction_error[factor][tau,t,policy,Ni,:] = v_depolarization.reshape(-1)\n",
    "                \n",
    "                # variational free energy for each policy\n",
    "                F_intermidiate = np.sum(Ft, axis=1) # sum over state factors\n",
    "                F_intermidiate = np.squeeze(np.sum(F_intermidiate, 1)).T # sum over tau then squeeze into 16x3 matrix\n",
    "                # store the value of the message pass at the last iteration into the variational free energy\n",
    "                F_intermidiate_flatten = F_intermidiate.flatten()\n",
    "                F[policy, t] = F_intermidiate_flatten[np.flatnonzero(F_intermidiate)[-1]]\n",
    "\n",
    "            # Expected free energy (G) under each policy\n",
    "            # -------------------------------------------------------------------------------\n",
    "            \n",
    "            # Initialize intermediate expected free energy variable for each policy\n",
    "            G_intermediate = np.zeros((self.num_policies,1))\n",
    "            # Policy horizon for 'counterfactual rollout' for deep policies\n",
    "            horizon = self.T\n",
    "            \n",
    "            # Do the loop through policies \n",
    "            for policy in range(self.num_policies):\n",
    "                \n",
    "                # Bayesian superise about 'd'\n",
    "                if hasattr(self, 'd'):\n",
    "                    for factor in range(self.num_factors):\n",
    "                        G_intermediate[policy] = G_intermediate[policy] - np.dot(d_complexity[factor].reshape(-1), state_posterior[factor][policy,:,0].reshape(-1))\n",
    "                \n",
    "                # We then come to calculate the expected free energy from time t to the policy horizon.\n",
    "                for timestep in range(t, horizon):  \n",
    "                    # store the expected states from each policy and time\n",
    "                    for factor in range(self.num_factors):\n",
    "                        expected_states[factor] = state_posterior[factor][policy,:,timestep].reshape(-1,1)\n",
    "                        \n",
    "                    # calculate Bayesian surprise then add it the expected free energy\n",
    "                    G_intermediate[policy] = G_intermediate[policy] + self._G_epistemic_value(A[:], expected_states[:])\n",
    "                    \n",
    "                    for modality in range(num_modalities):\n",
    "                        predictive_observations_posterior = self._cell_md_dot(A[modality], expected_states)\n",
    "                        G_intermediate[policy] = G_intermediate[policy] + (predictive_observations_posterior.T).dot(C[modality][:,t])\n",
    "            \n",
    "            G[:,t] = G_intermediate.squeeze()\n",
    "\n",
    "            # Inferring policy, updating precision and calculating BMA over policies\n",
    "            # -------------------------------------------------------------------------------\n",
    "            if t > 0:\n",
    "                gamma[t] = gamma[t-1]\n",
    "            \n",
    "            for ni in range(num_iterations):\n",
    "                # we here calculate prior and posterior over policies\n",
    "                policy_priors[:,t] = np.exp(np.log(E).reshape(-1)+gamma[t]*G[:,t])/np.sum(np.exp(np.log(E).reshape(-1)+gamma[t]*G[:,t])) # prior over policies\n",
    "                policy_posteriors[:,t] = np.exp(np.log(E).reshape(-1)+gamma[t]*G[:,t]+F[:,t])/sum(np.exp(np.log(E).reshape(-1)+gamma[t]*G[:,t]+F[:,t])) # posterior over policies\n",
    "\n",
    "                # calculations of beta (precision of expected free energy)\n",
    "                beta_update = (policy_posteriors[:,t]-policy_priors[:,t]).dot(G[:,t])\n",
    "                dFd_gamma = posterior_beta - self.beta + beta_update\n",
    "                posterior_beta = posterior_beta - dFd_gamma/2\n",
    "                gamma[t] = 1/posterior_beta\n",
    "\n",
    "                # dopamine responses simulations\n",
    "                n = t*num_iterations + ni\n",
    "                gamma_update[n] = gamma[t] # neural encoding of precision simulated at each iteration of variational update\n",
    "                policy_posterior_updates[:,n] = policy_posteriors[:,t] # neural encoding of policy posteriors\n",
    "                policy_posterior[:,t] = policy_posteriors[:,t]\n",
    "                \n",
    "            # BMA of hidden states (average over policies)\n",
    "            for factor in range(self.num_factors):\n",
    "                BMA_state[factor] = np.zeros((num_states[factor], self.T))\n",
    "                for tau in range(self.T):\n",
    "                    BMA_state[factor][:,tau] = np.matmul((state_posterior[factor][:,:,tau].T),(policy_posteriors[:,t]))\n",
    "            \n",
    "            # Action selection\n",
    "            # -------------------------------------------------------------------------------\n",
    "            if t < self.T-1:\n",
    "                # marginal posterior over action\n",
    "                action_posterior_intermediate = np.zeros(num_controllable_transitions[-1])\n",
    "                \n",
    "                for policy in range(self.num_policies):\n",
    "                    sub = [self.V[i][t,policy] for i in range(len(self.V))]\n",
    "                    action_posterior_intermediate[sub[-1]-1] = action_posterior_intermediate[sub[-1]-1] + policy_posteriors[policy,t]\n",
    "                \n",
    "                # action selection\n",
    "                action_posterior_intermediate = np.exp(self.alpha*np.log(action_posterior_intermediate+np.exp(-16)))/np.sum(np.exp(self.alpha*np.log(action_posterior_intermediate+np.exp(-16))))\n",
    "                action_posterior[t][0,:] = action_posterior_intermediate\n",
    "                \n",
    "                # next action selection\n",
    "                control_index_list = [index for index,value in enumerate(num_controllable_transitions) if value > 1]\n",
    "                control_index = control_index_list[0]\n",
    "                action = [i for i in range(1, num_controllable_transitions[control_index]+1)]\n",
    "                for factor in range(self.num_factors):\n",
    "                    if num_controllable_transitions[factor] > 2: # if there is more than one control state\n",
    "                        ind = np.where(np.random.rand() < np.cumsum(action_posterior_intermediate))[0][0]\n",
    "                        self.chosen_action[factor,t] = action[ind]\n",
    "\n",
    "\n",
    "        # Accumulate concentration parameters\n",
    "        # -------------------------------------------------------------------------------\n",
    "        # Initial hidden states 'd'\n",
    "        if hasattr(self, 'd'):\n",
    "            for factor in range(self.num_factors):\n",
    "                index_list = 1*(self.d[factor].reshape(-1) > 0)\n",
    "                index = np.where(index_list>0)[0][0]\n",
    "                self.d[factor][index] = self.omega*self.d[factor][index] + self.eta*BMA_state[factor][index,0]\n",
    "                #print(d_prior)\n",
    "        \n",
    "        # Free energy of concentration parameters\n",
    "        # -------------------------------------------------------------------------------\n",
    "        # Negative free energy of 'd'.\n",
    "        # Basically we are calculating the KL difference of 'd' after and before learning\n",
    "        Fd = np.zeros(self.num_factors)\n",
    "        for factor in range(self.num_factors):\n",
    "            if hasattr(self, 'd'):\n",
    "                Fd[factor] = - self._spm_KL_dir(self.d[factor], d_prior[factor])\n",
    "        setattr(self, 'Fd', Fd)\n",
    "\n",
    "        # Simulation of dopamine responses\n",
    "        # -------------------------------------------------------------------------------\n",
    "        if self.num_policies > 1:\n",
    "            phasic_dopamine = 8*np.gradient(gamma_update) + gamma_update/8\n",
    "        else:\n",
    "            phasic_dopamine = []\n",
    "            gamma_update = []\n",
    "        \n",
    "        # BMA of neuronal variables; normalized firing rates and prediction error\n",
    "        BMA_normalized_firing_rates = [0 for i in range(self.num_factors)]\n",
    "        BMA_prediction_error = [0 for i in range(self.num_factors)]\n",
    "\n",
    "        for factor in range(self.num_factors):\n",
    "            BMA_normalized_firing_rates[factor] = np.zeros((self.T, self.T, num_iterations, num_states[factor]))\n",
    "            BMA_prediction_error[factor] = np.zeros((self.T, self.T, num_iterations, num_states[factor]))\n",
    "            for t in range(self.T):\n",
    "                for policy in range(self.num_policies):\n",
    "                    BMA_normalized_firing_rates[factor][0:self.T,t,:,:] = BMA_normalized_firing_rates[factor][0:self.T,t,:,:] + normalized_firing_rates[factor][0:self.T,t,policy,:,:]*policy_posterior[policy,t]\n",
    "                    BMA_prediction_error[factor][0:self.T,t,:,:] = BMA_prediction_error[factor][0:self.T,t,:,:] + prediction_error[factor][0:self.T,t,policy,:,:]*policy_posterior[policy,t]\n",
    "\n",
    "        # Store variables into the class\n",
    "        # -------------------------------------------------------------------------------\n",
    "        setattr(self, 'O', O) # outcomes\n",
    "        setattr(self, 'P', action_posterior) # probability of action at time 1,...,T - 1\n",
    "        setattr(self, 'R', policy_posterior) # posterior over policies\n",
    "        setattr(self, 'Q', state_posterior) # conditional expectations over N states\n",
    "        setattr(self, 'X', BMA_state) # Bayesian model averages over T outcomes\n",
    "        setattr(self, 'C', C) # preferences\n",
    "        setattr(self, 'F', F) # variation free energy\n",
    "        setattr(self, 'G', G) # expected free energy\n",
    "        \n",
    "        setattr(self, 's', true_states)\n",
    "        setattr(self, 'o', outcomes)\n",
    "        setattr(self, 'u', self.chosen_action)\n",
    "\n",
    "        setattr(self, 'w', gamma) # posterior expectations of expected free energy precision\n",
    "        setattr(self, 'vn', BMA_prediction_error[:]) # simulated prediction error\n",
    "        setattr(self, 'xn', BMA_normalized_firing_rates[:]) # simulated neuronal encoding of hidden states\n",
    "        setattr(self, 'un', policy_posterior_updates) # simulated neuronal encoding of policies\n",
    "        setattr(self, 'wn', gamma_update) # simulated neuronal encoding of policy precision\n",
    "        setattr(self, 'dn', phasic_dopamine) # simulated dopamine responses\n",
    "        \n",
    "    # Inner methods\n",
    "    # -------------------------------------------------------------------------------\n",
    "    # Normalize vector columns\n",
    "    def _col_norm(self, input_matrices):\n",
    "        num_factors = len(input_matrices)\n",
    "        output_matrices = [np.zeros(input_matrices[i].shape) for i in range(num_factors)]\n",
    "        for i in range(num_factors):\n",
    "            num = input_matrices[i].shape\n",
    "            if num[1] == 1:\n",
    "                z = np.sum(input_matrices[i], axis=0)\n",
    "                output_matrices[i] = input_matrices[i]/z\n",
    "            else:\n",
    "                for j in range(num[0]):\n",
    "                    z = np.sum(input_matrices[i][j], axis=0)\n",
    "                    input_shape = input_matrices[i][j].shape\n",
    "                    if (input_matrices[i][j] - np.zeros(input_shape) == np.zeros(input_shape)).all():\n",
    "                        continue\n",
    "                    output_matrices[i][j] = input_matrices[i][j]/z\n",
    "            # output_matrices[i] = input_matrices[i]\n",
    "        return output_matrices\n",
    "\n",
    "    # This function substracts the inverse of each column entry\n",
    "    # from the inverse of the sum of the columns and then divide by 2.\n",
    "    def _spm_wnorm(self, input_matrix):\n",
    "        output_matrix = input_matrix + np.exp(-16)\n",
    "        output_matrix = (1/np.sum(output_matrix, axis=0) - 1/output_matrix)/2\n",
    "        return output_matrix\n",
    "    \n",
    "    # Natural log that replaces zero values with very small values for numerical reasons.\n",
    "    def _nat_log(self, x):\n",
    "        return np.log(x+np.exp(-16))\n",
    "    \n",
    "    # Dot product along dimension f\n",
    "    def _md_dot(self, A, s, f):\n",
    "        if f == 0:\n",
    "            matrix_to_return = np.matmul(A.T, s)\n",
    "        elif f == 1:\n",
    "            matrix_to_return = np.matmul(A, s)\n",
    "        return matrix_to_return\n",
    "    \n",
    "    def _cell_md_dot(self, X, x):\n",
    "        DIM = np.array([i for i in range(len(x))]).reshape(1,len(x)) + np.ndim(X) - len(x)\n",
    "        for d in range(len(x)):\n",
    "            s = [1 for i in range(np.ndim(X))]\n",
    "            s[DIM[0][d]] = np.prod(list(x[d].shape))\n",
    "            s = s[2:] + s[0:2] # because numpy and matlab has different arrangement for the shape of matrices,we have consider this fact when reshaping numpy arrays.\n",
    "            x[d] = x[d].reshape(*s)\n",
    "            X = X*(x[d].reshape(*s))\n",
    "            if DIM[0][d]%2 != 0:\n",
    "                X = np.expand_dims(np.sum(X, axis=DIM[0][d]+1), axis=DIM[0][d]+1)\n",
    "            else:\n",
    "                X = np.expand_dims(np.sum(X, axis=0), axis=0)\n",
    "            #X = np.expand_dims(np.sum(X, axis=DIM[0][d]+1), axis=DIM[0][d]+1)\n",
    "        X = X.squeeze()\n",
    "        return X\n",
    "\n",
    "    # Normalize the elements of B transpose as required by MMP\n",
    "    def _B_norm(self, B):\n",
    "        bb = B\n",
    "        z = np.sum(bb, axis=0)\n",
    "        bb = bb/(z+np.exp(-16))\n",
    "        np.nan_to_num(bb, copy=False, nan=0)\n",
    "        b = bb\n",
    "        return b\n",
    "    \n",
    "    # Calculate the Bayesian surprise in expected free energy\n",
    "    def _G_epistemic_value(self, A, s):\n",
    "        qx = self._spm_cross(s)\n",
    "        qx_flatten = qx.flatten()\n",
    "        G = 0\n",
    "        qo = 0\n",
    "        for i in np.where(qx_flatten>np.exp(-16))[0]:\n",
    "            po = 1\n",
    "            for g in range(len(A)):\n",
    "                A_list = [A[g][i][:,j] for i in range(A[g].shape[0]) for j in range(A[g].shape[-1])]\n",
    "                po = np.kron(po,A_list[i])\n",
    "            po = po[:]\n",
    "            qo = qo + qx_flatten[i]*po\n",
    "            G = G + qx_flatten[i]*po.T.dot(self._nat_log(po))\n",
    "        G = G - qo.T.dot(self._nat_log(qo))\n",
    "        return G\n",
    "    \n",
    "    #  The KL divergence between two vectors\n",
    "    def _spm_KL_dir(self, q, p):\n",
    "        d = self._spm_betaln(p) - self._spm_betaln(q) - np.sum((p-q)*self._spm_psi(q+1/32))\n",
    "        d = np.sum(d)\n",
    "        return d\n",
    "\n",
    "    # This function performs normalization on a probability transition rate matrix\n",
    "    def _spm_psi(self, A):\n",
    "        B = psi(A) - psi(np.sum(A))\n",
    "        return B\n",
    "\n",
    "    # This function returns the multivariate beta function of a vector.\n",
    "    # Notice here that I only implemented the case of vector, which is the only case I need.\n",
    "    def _spm_betaln(self, z):\n",
    "        if z.shape[0] == 1 or z.shape[1] == 1:\n",
    "            y = np.sum(sc.gammaln(z+np.exp(-16))) - sc.gammaln(np.sum(z))\n",
    "        return y \n",
    "            \n",
    "    # The outer product of matrices.\n",
    "    # This method is buggy when dealing with 1-d arrays.\n",
    "    def _spm_cross(self, *args):\n",
    "        \n",
    "        # dealing with the single case\n",
    "        def helper_single(*args):\n",
    "            if len(args) == 1:\n",
    "                matrices = args[0]\n",
    "                extension_dims = [1 for i in range(len(args[0]))]\n",
    "                if isinstance(matrices, np.ndarray):\n",
    "                    Y = matrices\n",
    "                elif len(matrices) <= 1:\n",
    "                    Y = matrices[0]\n",
    "                else:\n",
    "                    Z = matrices.pop(0)\n",
    "                    Z_shape = list(Z.shape)\n",
    "                    if sum(extension_dims) == fixed_length:\n",
    "                        matrices = matrices[::-1]\n",
    "                    W = helper_single(*[matrices])\n",
    "                    W_shape = list(W.shape)\n",
    "                    W = W.reshape(*W_shape,*list(np.ones(np.ndim(Z),dtype=int)))\n",
    "                    Y = np.kron(W,Z)\n",
    "                return Y.squeeze()\n",
    "        \n",
    "        # dealing with multiple inputs\n",
    "        def helper_multiple(*args):\n",
    "            Y_list = []\n",
    "            for i in range(len(args)):\n",
    "                matrices = args[i]\n",
    "                extension_dims = [1 for i in range(len(matrices))]\n",
    "                if isinstance(matrices, np.ndarray):\n",
    "                    Y = matrices\n",
    "                elif len(matrices) <= 1:\n",
    "                    Y = matrices[0]\n",
    "                else:\n",
    "                    Z = matrices.pop(len(matrices)-1)\n",
    "                    Z_shape = list(Z.shape)\n",
    "                    if sum(extension_dims) == fixed_length_list[i]:\n",
    "                        matrices = matrices[::-1]\n",
    "                    W = helper_single(*[matrices])\n",
    "                    Z = Z.reshape(*Z.shape,*list(np.ones(np.ndim(W),dtype=int)))\n",
    "                    W_shape = list(W.shape)\n",
    "                    Y = np.kron(W,Z)\n",
    "                Y_list.append(Y)\n",
    "            V = self._spm_cross(Y_list)\n",
    "            return V\n",
    "\n",
    "        if len(args) == 1:\n",
    "            fixed_length = len(args[0])\n",
    "            return helper_single(*args)\n",
    "        else:\n",
    "            fixed_length_list = [len(args[i]) for i in range(len(args))]\n",
    "            return helper_multiple(*args)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "source": [
    "MDP = MDP.explore_exploit_model()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "source": [
    "MDP.message_passing_and_policy_selection()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "MDP.C"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array([[-1.09861195, -1.09861195, -1.09861195],\n",
       "        [-1.09861195, -1.09861195, -1.09861195],\n",
       "        [-1.09861195, -1.09861195, -1.09861195]]),\n",
       " array([[-1.09861195, -4.02473859, -2.16984503],\n",
       "        [-1.09861195, -5.02472777, -3.16984334],\n",
       "        [-1.09861195, -0.02474477, -0.16984589]]),\n",
       " array([[-1.38629391, -1.38629391, -1.38629391],\n",
       "        [-1.38629391, -1.38629391, -1.38629391],\n",
       "        [-1.38629391, -1.38629391, -1.38629391],\n",
       "        [-1.38629391, -1.38629391, -1.38629391]])]"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "647b363b96b61335887d6dcfd84963b145589598b0b93027d57d6af73bddc64c"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('tf': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}